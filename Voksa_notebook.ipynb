{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing and Importing the module and libraries\n",
        "\n",
        "First, in the initial code cell, we're making sure we have all the necessary tools by installing some Python packages. The first line fetches a package from a specific GitHub repository, and the second line installs three other important packages: `transformers`, `accelerate`, and `safetensors`. These packages provide us with the libraries and functionalities we need for our project."
      ],
      "metadata": {
        "id": "5I3uMS9zyqnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers\n",
        "!pip install transformers accelerate safetensors"
      ],
      "metadata": {
        "id": "GNcUG44Nq3Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the Transformers and StableDiffusion\n",
        "\n",
        "The second code cell sets up the environment for generating images. It imports required modules, creates a `StableDiffusionXLPipeline`, and configures it to use the GPU for faster processing. This pipeline is a key component for generating the desired Volkswagen image with innovative front-end features."
      ],
      "metadata": {
        "id": "4lMtQzG1ytCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionXLPipeline\n",
        "import torch\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\"segmind/SSD-1B\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
        "pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "PzC3TwUnq9Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying lora to make model a bit small\n",
        "\n",
        "First, we're setting up some crucial information using environment variables. `MODEL_NAME` is set to \"segmind/SSD-1B\", VAE_NAME to \"madebyollin/sdxl-vae-fp16-fix\", and `DATASET_NAME` to \"dataset/deepvisualmarketing\". These variables hold specific paths and identifiers for the models and dataset we'll be working with.\n",
        "\n",
        "Next, we're launching a Python script called `train_text_to_image_sdxl.py` using the accelerate command. This script is likely responsible for training a model to generate images from text prompts. We're passing in a bunch of arguments to fine-tune this process. For instance, we're enabling a memory-efficient attention mechanism, specifying image resolution, and applying augmentation techniques like random flipping.\n",
        "\n",
        "We're also controlling the batch size for training, handling gradient accumulation, and utilizing gradient checkpointing for memory efficiency. The `max_train_steps` parameter sets the upper limit on training steps, and we're opting for an 8-bit Adam optimizer for faster training. Learning rate settings, mixed precision training, and reporting progress to Weights & Biases are all part of the configuration.\n",
        "\n",
        "We're introducing a validation prompt and defining how often we'll check the model's performance during training. Model checkpoints will be saved every 5000 steps, and the training outputs will be stored in a directory named \"sdxl-car-model\". Lastly, the command `--push_to_hub` indicates that the trained model will be shared in a repository for further use."
      ],
      "metadata": {
        "id": "d8gd6K9uz9tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export MODEL_NAME=\"segmind/SSD-1B\"\n",
        "export VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\n",
        "export DATASET_NAME=\"dataset/deepvisualmarketing\"\n",
        "\n",
        "accelerate launch train_text_to_image_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n",
        "  --dataset_name=$DATASET_NAME \\\n",
        "  --enable_xformers_memory_efficient_attention \\\n",
        "  --resolution=512 --center_crop --random_flip \\\n",
        "  --proportion_empty_prompts=0.2 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 --gradient_checkpointing \\\n",
        "  --max_train_steps=10000 \\\n",
        "  --use_8bit_adam \\\n",
        "  --learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --report_to=\"wandb\" \\\n",
        "  --validation_prompt=\"a volksvagen car with inverted doors\" --validation_epochs 5 \\\n",
        "  --checkpointing_steps=5000 \\\n",
        "  --output_dir=\"sdxl-car-model\" \\\n",
        "  --push_to_hub"
      ],
      "metadata": {
        "id": "0TLzKkU_z-nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning it on the [Deep Visual](https://deepvisualmarketing.github.io/)\n",
        "\n",
        "Now, we get into some more advanced stuff. We're talking about fine-tuning a model, which means we're customizing it for our specific needs. We're using environment variables to specify which models and datasets we're working with. Then, we're using a command called `accelerate` to run a Python script named `train_text_to_image_lora_sdxl.py`. This script is where the model gets fine-tuned, using the models and data we've specified."
      ],
      "metadata": {
        "id": "Kd8wFASkyvAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export MODEL_NAME=\"segmind/SSD-1B\"\n",
        "export VAE_NAME=\"madebyollin/sdxl-vae-fp16-fix\"\n",
        "export DATASET_NAME=\"dataset/deepvisualmarketing\"\n",
        "\n",
        "accelerate launch train_text_to_image_lora_sdxl.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_model_name_or_path=$VAE_NAME \\\n",
        "  --dataset_name=$DATASET_NAME --caption_column=\"text\" \\\n",
        "  --resolution=1024 --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=2 --checkpointing_steps=500 \\\n",
        "  --learning_rate=1e-04 --lr_scheduler=\"constant\" --lr_warmup_steps=0 \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --seed=42 \\\n",
        "  --output_dir=\"sd-car-model-lora-sdxl\" \\\n",
        "  --validation_prompt=\"A Volksvagen car with extended hatachback\" --report_to=\"wandb\" \\\n",
        "  --push_to_hub"
      ],
      "metadata": {
        "id": "6GAkQwUPte-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running and testing the model\n",
        "\n",
        "Finally, we're actually generating the image. We're giving the model a detailed description of what we want the redesigned Volkswagen to look like, down to specific features like headlights and grille. We're also providing a negative prompt to help guide the model away from undesired outcomes. The `pipe` object takes these instructions and produces the image, which we then display for review"
      ],
      "metadata": {
        "id": "mY3XyM9wyx47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Create an image of a redesigned Volkswagen with futuristic front-end features. The design should blend modern aesthetics with classic Volkswagen elements, showcasing innovative headlights, a sleek grille, and dynamic lines that give the car a distinctive and forward-looking appearance\" # Your prompt here\n",
        "neg_prompt = \"ugly, blurry, poor quality\"\n",
        "image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\n",
        "display(image)"
      ],
      "metadata": {
        "id": "O1s4jVZBrYg6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}